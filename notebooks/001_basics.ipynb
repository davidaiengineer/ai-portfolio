{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Portfolio - Day 1 Basics\n",
        "\n",
        "This notebook serves as a refresher for fundamental AI/ML concepts and sets up the foundation for our portfolio projects.\n",
        "\n",
        "## Learning Objectives\n",
        "- Review pandas and data manipulation basics\n",
        "- Load and explore a simple dataset\n",
        "- Train a basic machine learning model\n",
        "- Understand the project structure and workflow\n",
        "\n",
        "## Table of Contents\n",
        "1. [Environment Setup](#environment-setup)\n",
        "2. [Data Loading and Exploration](#data-loading-and-exploration)\n",
        "3. [Basic Data Analysis](#basic-data-analysis)\n",
        "4. [Simple ML Model](#simple-ml-model)\n",
        "5. [Next Steps](#next-steps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "First, let's import the necessary libraries and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")\n",
        "print(f\"üìä Pandas version: {pd.__version__}\")\n",
        "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
        "print(f\"üìà Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"üé® Seaborn version: {sns.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Exploration\n",
        "\n",
        "Let's create a synthetic dataset to practice our data manipulation skills.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=8,\n",
        "    n_redundant=2,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create DataFrame\n",
        "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "print(\"üìä Dataset Overview:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Features: {df.shape[1] - 1}\")\n",
        "print(f\"Samples: {df.shape[0]}\")\n",
        "print(f\"Classes: {df['target'].nunique()}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nüîç First 5 rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Data Analysis\n",
        "\n",
        "Let's explore our dataset with some basic statistics and visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"üìà Dataset Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\nüéØ Target Distribution:\")\n",
        "print(df['target'].value_counts().sort_index())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n‚ùì Missing Values:\")\n",
        "print(df.isnull().sum().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Target distribution\n",
        "df['target'].value_counts().plot(kind='bar', ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Target Distribution')\n",
        "axes[0, 0].set_xlabel('Class')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "\n",
        "# Feature correlation heatmap\n",
        "correlation_matrix = df[feature_names].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Feature Correlation Matrix')\n",
        "\n",
        "# Feature distributions\n",
        "df[feature_names[:4]].hist(bins=20, ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Feature Distributions (First 4)')\n",
        "\n",
        "# Box plot for feature_0 by target\n",
        "df.boxplot(column='feature_0', by='target', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Feature 0 by Target Class')\n",
        "axes[1, 1].set_xlabel('Target Class')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple ML Model\n",
        "\n",
        "Now let's train a basic machine learning model to get familiar with the scikit-learn workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for modeling\n",
        "X = df[feature_names]\n",
        "y = df['target']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"üìä Training set size: {X_train.shape[0]}\")\n",
        "print(f\"üìä Test set size: {X_test.shape[0]}\")\n",
        "print(f\"üìä Feature count: {X_train.shape[1]}\")\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "print(\"\\n‚úÖ Model training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model evaluation\n",
        "print(\"üìä Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.xlabel('Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Great! You've completed the basics notebook. Here's what we've covered:\n",
        "\n",
        "### ‚úÖ What We Learned\n",
        "1. **Environment Setup**: Imported essential libraries and set up reproducible environments\n",
        "2. **Data Manipulation**: Created and explored a synthetic dataset using pandas\n",
        "3. **Data Visualization**: Created various plots to understand data distributions and relationships\n",
        "4. **Machine Learning**: Trained a Random Forest classifier and evaluated its performance\n",
        "5. **Model Interpretation**: Analyzed feature importance and model predictions\n",
        "\n",
        "### üöÄ Next Steps\n",
        "1. **P1 - RAG Project**: Start building your retrieval-augmented generation system\n",
        "2. **P2 - Vision Project**: Work on computer vision and multimodal models\n",
        "3. **P3 - MLOps Project**: Create production-ready ML services with proper CI/CD\n",
        "\n",
        "### üìö Key Takeaways\n",
        "- Always set random seeds for reproducibility\n",
        "- Explore your data thoroughly before modeling\n",
        "- Use proper train/test splits with stratification\n",
        "- Evaluate models with multiple metrics\n",
        "- Visualize results for better understanding\n",
        "\n",
        "### üîó Resources\n",
        "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
        "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
        "- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/)\n",
        "- [Seaborn Examples](https://seaborn.pydata.org/examples/)\n",
        "\n",
        "Happy coding! üéâ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
